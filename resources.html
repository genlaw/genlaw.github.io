<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="icon" type="image/svg+xml" href="/favicon.svg">
    <link rel="icon" type="image/png" href="/favicon.png">
    

    <title>GenLaw ’23: Resources</title>
        <link rel="stylesheet" href="styles.css">
    
    <!-- Google Analytics tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W2ZW2ZM1M6"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-W2ZW2ZM1M6');
    </script>
</head>
<body>
    <header>
        <img src="/favicon.ico" alt="reading robot icon">
        <a href="index.html" style="white-space: nowrap;">GenLaw '23</a>
        <nav>
            <a href="index.html#cfp">CFP</a>
            <!-- <a href="schedule.html">Schedule & Speakers</a> -->
            <a href="index.html#contact-us">Contact</a>
            <a href="resources.html">Resources</a>
        </nav>
    </header>
    <main id="resources">
        <h1>GenLaw ’23: Resources</h1>
                <div id="table-of-contents">
            <ul><li><a href="#intellectual-property">Intellectual Property</a></li><li><a href="#privacy">Privacy</a></li><li><a href="#language-models">Language Models</a></li><li><a href="#diffusion-models">Diffusion Models</a></li><li><a href="#training-data-extraction">Training data extraction</a></li><li><a href="#membership-inference">Membership Inference</a></li><li><a href="#differential-privacy">Differential Privacy</a></li><li><a href="#ethics">Ethics</a></li><li><a href="#ongoing-litigation">Ongoing litigation</a></li><li><a href="#in-the-news">In the news</a></li></ul>
        </div>
                <h2 id="intellectual-property">Intellectual Property</h2><p><a href="https://www.bensobel.org/files/articles/41.1_Sobel-FINAL.pdf">Artificial Intelligence’s Fair Use Crisis</a> by Ben Sobel (2017)</p><p><a href="https://arxiv.org/abs/2303.15715">Foundation Models and Fair Use</a> by Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang (2023)</p><p><a href="https://public-inspection.federalregister.gov/2023-05321.pdf">LIBRARY OF CONGRESS Copyright Office 37 CFR Part 202 Copyright Registration Guidance: Works Containing Material Generated by Art</a> (2023)</p><p><a href="https://dl.acm.org/doi/pdf/10.1145/3486628?casa_token=iePHolse-r0AAAAA:iqzahhlIQzKwIiABJMSuVlfWoCYCzzSm_hHDneSUKUFMqdtDb8D8uE9zlwo_vEoNq9ygaoa9694Wtg">Legally Speaking Text and Data Mining of In-Copyright Works: Is It Legal?</a> by Pamela Samuelson (2021)</p><p><a href="https://texaslawreview.org/fair-learning/">Fair Learning</a> by Mark A. Lemley and Bryan Casey (2021)</p><p><a href="https://digitalcommons.law.uw.edu/wlr/vol93/iss2/2">How Copyright Law Can Fix Artificial Intelligence’s Implicit Bias Problem</a> by Amanda Levendowski (2018)</p><p><a href="https://james.grimmelmann.net/files/articles/computer-authored-works.pdf">There’s No Such Thing as a Computer-Authored Work—And It’s a Good Thing, Too</a> by James Grimmelmann (2016)</p><p><a href="https://blog.tidelift.com/resilient-open-commons">Resilient open commons</a> by Luis Villa (2022)</p><p><a href="https://arxiv.org/abs/2206.01230">Formalizing Human Ingenuity: A Quantitative Framework for Copyright Law’s Substantial Similarity</a> by Sarah Scheffler, Eran Tromer, and Mayank Varia (2022)</p><p><a href="https://arxiv.org/abs/2302.10870">Provable Copyright Protection for Generative Models</a> by Nikhil Vyas, Sham Kakade, and Boaz Barak (2023)</p><h2 id="privacy">Privacy</h2><p><a href="https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?article=1376&amp;context=penn_law_review">A taxonomy of privacy</a> by Daniel Solove (2006)</p><p><a href="https://heinonline.org/HOL/LandingPage?handle=hein.journals/bulr102&amp;div=20&amp;id=&amp;page=">Privacy Harms</a> by Danielle Citron and Daniel Solove (2022)</p><p><a href="https://heinonline.org/HOL/LandingPage?handle=hein.journals/calr107&amp;div=51&amp;id=&amp;page=">Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security</a> by Bobby Chesney and Danielle Citron (2019)</p><p><a href="https://www.sup.org/books/title/?id=8862">Privacy in Context: Technology, Policy, and the Integrity of Social Life</a> by Helen Nissenbaum (2009)</p><p><a href="https://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1506&amp;context=nulr">Information Privacy and the Inference Economy</a> by Alicia Solow-Niederman (2022)</p><p><a href="https://people.ischool.berkeley.edu/~pam/papers/privasip_draft.pdf">Privacy As Intellectual Property?</a> by Pamela Samuelson (2000)</p><dl><dt><a href="https://dl.acm.org/doi/fullHtml/10.1145/3531146.3534642">What Does it Mean for a Language Model to Preserve Privacy?</a> by Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr (2022)</dt><dd>Language models use unstructured text data which means private information is nebulous and also unstructured.</dd></dl><h2 id="language-models">Language Models</h2><dl><dt><a href="https://arxiv.org/abs/2304.00612">Eight Things to Know about Large Language Models</a> by Sam Bowman (2023)</dt><dd>Good introduction to LMs</dd><dt><a href="https://huggingface.co/course/chapter1/1">HuggingFace NLP Course</a></dt><dd>A more detailed set of tutorials on what NLP is, what Transformers are, and some more details about training/using language models.</dd><dt><a href="https://arxiv.org/abs/2002.12327">A Primer in BERTology: What we know about how BERT works</a> by Anna Rogers, Olga Kovaleva, and Anna Rumshisky (2020)</dt><dd>A fairly comprehensive review of the BERT language models that create embeddings of text.</dd><dt><a href="https://www.promptingguide.ai/">Prompting Guide</a> by DAIR.AI (2022)</dt><dd>Great guide on prompting</dd></dl><p><a href="https://www.bertforhumanists.org/tutorials/">BERT for Humanists</a> by Matt Wilkens, David Mimno, Melanie Walsh, and Rosamond Thalken (2022)</p><h2 id="diffusion-models">Diffusion Models</h2><p><a href="https://sander.ai/2023/01/09/diffusion-language.html">Diffusion language models</a> by Sander Dieleman (2023)</p><p><a href="https://sander.ai/2022/01/31/diffusion.html">Diffusion models are autoencoders</a> by Sander Dieleman (2022)</p><p><a href="https://scale.com/guides/diffusion-models-guide">Diffusion Models: A Practical Guide</a> by Vivek Muppalla and Sean Hendryx (2022)</p><dl><dt><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models</a> by Lilian Weng (2021)</dt><dd>A much more technical explanation of diffusion models.</dd><dt><a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a> by Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli (2015)</dt><dd>The original paper.</dd></dl><p><a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a> by Yang Song, Stefano Ermon (2019)</p><h2 id="training-data-extraction">Training data extraction</h2><dl><dt><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">Extracting Training Data from Large Language Models</a> by Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel (2021), <a href="https://bair.berkeley.edu/blog/2020/12/20/lmmem/" class="inline">blog</a>, <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting" class="inline">video</a></dt><dd>Original paper showcasing extracting training data from large language models.</dd><dt><a href="https://aclanthology.org/2022.acl-long.577/">Deduplicating Training Data Makes Language Models Better</a> by Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini (2022)</dt><dd>Duplicates in the data continue to be the most easily identifiable reason for memorization.</dd><dt><a href="https://arxiv.org/abs/2202.07646">Quantifying Memorization Across Neural Language Models</a> by Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang (2022)</dt><dd>A more careful and comprehensive study showing larger models memorize more.</dd><dt><a href="https://arxiv.org/abs/2301.13188">Extracting Training Data from Diffusion Models</a> by Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace (2023)</dt><dd>It’s also possible to extract training data from diffusion models.</dd></dl><h2 id="membership-inference">Membership Inference</h2><dl><dt><a href="https://ieeexplore.ieee.org/abstract/document/7958568">Membership Inference Attacks against Machine Learning Models</a> by Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov (2017)</dt><dd>First paper introducing the idea of membership inference.</dd><dt><a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles</a> by Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer (2021)</dt><dd>Current best membership inference attack.</dd><dt><a href="http://proceedings.mlr.press/v139/choquette-choo21a/choquette-choo21a.pdf">Label-Only Membership Inference Attacks</a> by Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot (2021)</dt><dd>Masking outputs does not prevent membership inference.</dd><dt><a href="https://arxiv.org/abs/1802.04889">Understanding Membership Inferences on Well-Generalized Learning Models</a> by Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen</dt><dd>Outliers can be more vulnerable to membership inference</dd></dl><h2 id="differential-privacy">Differential Privacy</h2><p><a href="https://proceedings.neurips.cc/paper/2020/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a> by Matthew Jagielski, Jonathan Ullman, and Alina Oprea (2020)</p><p><a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978318">Deep Learning with Differential Privacy</a> by Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang (2016)</p><p><a href="https://arxiv.org/abs/2201.12328">Toward Training at ImageNet Scale with Differential Privacy</a> by Alexey Kurakin, Shuang Song, Steve Chien, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta (2022)</p><dl><dt><a href="https://arxiv.org/abs/2212.06470">Considerations for Differentially Private Learning with Large-Scale Public Pretraining</a> by Florian Tramèr, Gautam Kamath, Nicholas Carlini (2022)</dt><dd>Privacy is hard. Publicly accessible data != public data. Differential privacy has limitations. Public data <em>looks</em> different from private data in meaningful ways, but our benchmarks sometimes miss that.</dd></dl><p><a href="https://aclanthology.org/2022.findings-acl.171.pdf">Training Text-to-Text Transformers with Privacy Guarantees</a> by Natalia Ponomareva Jasmijn Bastings Sergei Vassilvitskii (2022)</p><dl><dt><a href="https://arxiv.org/abs/2303.00654">How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy</a> by Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Thakurta (2023)</dt><dd>Practical guide to implementing DP.</dd><dt><a href="https://arxiv.org/abs/2211.06530">Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning</a> by Christopher A. Choquette-Choo, H. Brendan McMahan, Keith Rush, and Abhradeep Thakurta</dt><dd>State-of-the-art privacy mechanism.</dd></dl><h2 id="ethics">Ethics</h2><p><a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0083">Algorithms that remember: model inversion attacks and data protection law</a> by Michael Veale, Reuben Binns, and Lilian Edwards (2018)</p><p><a href="https://arxiv.org/abs/2112.04359">Ethical and social risks of harm from Language Models</a> by Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel (2021)</p><p><a href="https://arxiv.org/abs/2102.02503">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</a> by Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli (2021)</p><p><a href="https://dl.acm.org/doi/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</a> by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell (2021)</p><p><a href="https://arxiv.org/abs/2202.05338">Accountability in an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learning</a> by A. Feder Cooper, Emanuel Moss, Benjamin Laufer and Helen Nissenbaum (2022)</p><p><a href="https://dl.acm.org/doi/abs/10.1145/3311957.3359435">Contestability in Algorithmic Systems</a> by Kristen Vaccaro, Karrie Karahalios, Deirdre K. Mulligan, Daniel Kluttz, and Tad Hirsch (2019)</p><h2 id="ongoing-litigation">Ongoing litigation</h2><p><a href="https://githubcopilotlitigation.com/">Doe 1 et al. v. GitHub, Inc. et al.</a> GitHub Copilot class action lawsuit</p><p><a href="https://stablediffusionlitigation.com/">Andersen et al. v. Stability AI Ltd. et al.</a> Stable Diffusion class action lawsuit</p><h2 id="in-the-news">In the news</h2><p><a href="https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt">Samsung workers made a major error by using ChatGPT</a> by Lewis Maddison for <em>TechRadar</em> (2023)</p><p><a href="https://arstechnica.com/tech-policy/2023/04/openai-may-be-sued-after-chatgpt-falsely-says-aussie-mayor-is-an-ex-con/">OpenAI threatened with landmark defamation lawsuit over ChatGPT false claims</a> by Ashley Belanger for <em>Ars Technica</em> (2023)</p><p><a href="https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit">Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content</a> by James Vincent for <em>The Verge</em> (2023)</p><p><a href="https://ipkitten.blogspot.com/2023/02/uk-government-axes-plans-to-broaden.html">UK Government axes plans to broaden existing text and data mining exception</a> by Eleonora Rosati for <em>IPKat</em> (2023)</p><p><a href="https://copyrightblog.kluweriplaw.com/2022/08/24/the-uk-government-moves-forward-with-a-text-and-data-mining-exception-for-all-purposes/">The UK government moves forward with a text and data mining exception for all purposes</a> by Alina Trapova and João Pedro Quintais for <em>Kluwer Copyright Blog</em> (2022)</p><p><a href="https://www.project-disco.org/intellectual-property/011823-israel-ministry-of-justice-issues-opinion-supporting-the-use-of-copyrighted-works-for-machine-learning/">Israel Ministry of Justice Issues Opinion Supporting the Use of Copyrighted Works for Machine Learning</a> by Jonathan Band for <em>The Disruptive Competition Project</em> (2023)</p><p><a href="https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E">Copyright in generative deep learning</a> by Giorgio Franceschelli and Mirco Musolesi for <em>Data &amp; Policy</em> (2022)</p><p><a href="https://publishingperspectives.com/2023/03/ai-at-bologna-the-hair-raising-topic-of-2023/">‘AI’ at Bologna: The Hair-Raising Topic of 2023?</a> by Porter Anderson for <em>Publishing Perspectives</em> (2023)</p><p><a href="https://www2.law.ucla.edu/volokh/ailibel.pdf">Large Libel Models? Liability for AI Output</a> by Eugene Volokh (draft 2023)</p><p><a href="https://www.lawfareblog.com/section-230-wont-protect-chatgpt">Section 230 Won’t Protect ChatGPT</a> by Matt Perault (2023)</p><p><a href="https://www.technologyreview.com/2023/04/19/1071789/openais-hunger-for-data-is-coming-back-to-bite-it/">OpenAI’s hunger for data is coming back to bite it</a> by Melissa Heikkilä (2023)</p>
    </main>
    <footer>
        <p style="white-space: nowrap;">GenLaw '23</p>
        <p class="colophon">Built with <a href="http://pandoc.org">Pandoc</a> and <a href="https://chat.openai.com/chat?model=gpt-4">ChatGPT</a> :)</p>
    </footer>
</body>
</html>
