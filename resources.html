<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc-markdown-css-theme" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>GenLaw: Resources</title>
  <link rel="stylesheet" href="css/theme.css" />
  <link rel="stylesheet" href="css/skylighting-paper-theme.css" />
  <link rel="stylesheet" href="css/theme-additions.css" />
<!-- Google Analytics tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W2ZW2ZM1M6"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-W2ZW2ZM1M6');
</script>
</head>
<body>

<header>
  <h1 class="title">GenLaw: Resources</h1>
<div class="metadata">
</div>

</header>

<nav id="TOC" role="doc-toc">
    <input type="checkbox" id="contents">
  <label for="contents">
    <h4 id="toc-heading">
              Contents
            <svg id="toc-chevron" width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M2.08926 3.16074C1.76382 2.83531 1.23618 2.83531 0.910744 3.16074C0.585307 3.48618 0.585307 4.01382 0.910744 4.33926L2.08926 3.16074ZM6 8.25L5.41074 8.83926C5.73618 9.16469 6.26382 9.16469 6.58926 8.83926L6 8.25ZM11.0893 4.33926C11.4147 4.01382 11.4147 3.48618 11.0893 3.16074C10.7638 2.83531 10.2362 2.83531 9.91074 3.16074L11.0893 4.33926ZM0.910744 4.33926L5.41074 8.83926L6.58926 7.66074L2.08926 3.16074L0.910744 4.33926ZM6.58926 8.83926L11.0893 4.33926L9.91074 3.16074L5.41074 7.66074L6.58926 8.83926Z" fill="currentColor"/>
      </svg>
    </h4>
  </label>
  <ul>
  <li><a href="#intellectual-property" id="toc-intellectual-property">Intellectual Property</a></li>
  <li><a href="#privacy" id="toc-privacy">Privacy</a></li>
  <li><a href="#competition-and-anti-trust" id="toc-competition-and-anti-trust">Competition and Anti-Trust</a></li>
  <li><a href="#language-models" id="toc-language-models">Language Models</a></li>
  <li><a href="#diffusion-models" id="toc-diffusion-models">Diffusion Models</a></li>
  <li><a href="#training-data-extraction" id="toc-training-data-extraction">Training data extraction</a></li>
  <li><a href="#membership-inference" id="toc-membership-inference">Membership Inference</a></li>
  <li><a href="#differential-privacy" id="toc-differential-privacy">Differential Privacy</a></li>
  <li><a href="#ethics" id="toc-ethics">Ethics</a></li>
  <li><a href="#ongoing-litigation" id="toc-ongoing-litigation">Ongoing litigation</a></li>
  <li><a href="#in-the-news" id="toc-in-the-news">In the news</a></li>
  </ul>
</nav>


<main id="main" class="">
        <p style="text-align:right"><a href="https://genlaw.github.io/">Back to GenLaw ↩︎</a></p>
  <p style="text-align:right">
Also see the <a href="glossary.html">glossary</a>.
</p>
<h2 id="intellectual-property">Intellectual Property</h2>
<p><a href="https://www.cis.upenn.edu/~ccb/publications/understanding-generative-AI-and-its-relationship-to-copyright.pdf">Understanding Generative Artificial Intelligence and Its Relationship to Copyright</a> by Christopher Callison-Burch (May 2023) [<a href="https://www.youtube.com/live/Mm1NQ_Kqumw?feature=share&amp;t=887">video</a>]</p>
<p><a href="https://arxiv.org/abs/2303.15715">Foundation Models and Fair Use</a> by Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang (2023)</p>
<p><a href="https://www.bensobel.org/files/articles/41.1_Sobel-FINAL.pdf">Artificial Intelligence’s Fair Use Crisis</a> by Ben Sobel (2017)</p>
<p><a href="https://public-inspection.federalregister.gov/2023-05321.pdf">LIBRARY OF CONGRESS Copyright Office 37 CFR Part 202 Copyright Registration Guidance: Works Containing Material Generated by Art</a> (2023)</p>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3486628?casa_token=iePHolse-r0AAAAA:iqzahhlIQzKwIiABJMSuVlfWoCYCzzSm_hHDneSUKUFMqdtDb8D8uE9zlwo_vEoNq9ygaoa9694Wtg">Legally Speaking Text and Data Mining of In-Copyright Works: Is It Legal?</a> by Pamela Samuelson (2021)</p>
<p><a href="https://texaslawreview.org/fair-learning/">Fair Learning</a> by Mark A. Lemley and Bryan Casey (2021)</p>
<!-- [How Copyright Law Can Fix Artificial Intelligence's Implicit Bias Problem](https://digitalcommons.law.uw.edu/wlr/vol93/iss2/2) by Amanda Levendowski (2018) -->
<p><a href="https://james.grimmelmann.net/files/articles/computer-authored-works.pdf">There’s No Such Thing as a Computer-Authored Work—And It’s a Good Thing, Too</a> by James Grimmelmann (2016)</p>
<p><a href="https://blog.tidelift.com/resilient-open-commons">Resilient open commons</a> by Luis Villa (2022)</p>
<p><a href="https://arxiv.org/abs/2206.01230">Formalizing Human Ingenuity: A Quantitative Framework for Copyright Law’s Substantial Similarity</a> by Sarah Scheffler, Eran Tromer, and Mayank Varia (2022)</p>
<p><a href="https://arxiv.org/abs/2302.10870">Provable Copyright Protection for Generative Models</a> by Nikhil Vyas, Sham Kakade, and Boaz Barak (2023)</p>
<p><a href="https://scholarship.law.columbia.edu/cgi/viewcontent.cgi?article=3327&amp;context=faculty_scholarship">Authors and Machines</a> by Jane Ginsburg and Luke Ali Budiarjo (2019)</p>
<p><a href="https://people.ischool.berkeley.edu/~pam/papers/47UPittLRev1185.pdf">Allocating ownership rights in computer-generated works</a> by Pamela Samuelson (1985)</p>
<h2 id="privacy">Privacy</h2>
<p><a href="https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?article=1376&amp;context=penn_law_review">A taxonomy of privacy</a> by Daniel Solove (2006)</p>
<p><a href="https://heinonline.org/HOL/LandingPage?handle=hein.journals/bulr102&amp;div=20&amp;id=&amp;page=">Privacy Harms</a> by Danielle Citron and Daniel Solove (2022)</p>
<p><a href="https://heinonline.org/HOL/LandingPage?handle=hein.journals/calr107&amp;div=51&amp;id=&amp;page=">Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security</a> by Bobby Chesney and Danielle Citron (2019)</p>
<p><a href="https://www.sup.org/books/title/?id=8862">Privacy in Context: Technology, Policy, and the Integrity of Social Life</a> by Helen Nissenbaum (2009)</p>
<p><a href="https://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1506&amp;context=nulr">Information Privacy and the Inference Economy</a> by Alicia Solow-Niederman (2022)</p>
<p><a href="https://people.ischool.berkeley.edu/~pam/papers/privasip_draft.pdf">Privacy As Intellectual Property?</a> by Pamela Samuelson (2000)</p>
<dl>
<dt><a href="https://dl.acm.org/doi/fullHtml/10.1145/3531146.3534642">What Does it Mean for a Language Model to Preserve Privacy?</a> by Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr (2022)</dt>
<dd>
Language models use unstructured text data which means private information is nebulous and also unstructured.
</dd>
</dl>
<h2 id="competition-and-anti-trust">Competition and Anti-Trust</h2>
<p><a href="https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/06/generative-ai-raises-competition-concerns">Generative AI Raises Competition Concerns</a> by the FTC Staff (2023)</p>
<h2 id="language-models">Language Models</h2>
<dl>
<dt><a href="https://arxiv.org/abs/2304.00612">Eight Things to Know about Large Language Models</a> by Sam Bowman (2023)</dt>
<dd>
Good introduction to LMs.
</dd>
</dl>
<p><a href="https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e">A Very Gentle Introduction to Large Language Models without the Hype</a> by Mark Riedl (2023)</p>
<p><a href="https://vickiboykis.com/what_are_embeddings/about.html">What are embeddings?</a> by Vicky Boykis (2023)</p>
<dl>
<dt><a href="https://huggingface.co/course/chapter1/1">HuggingFace NLP Course</a></dt>
<dd>
A more detailed set of tutorials on what NLP is, what Transformers are, and some more details about training/using language models.
</dd>
<dt><a href="https://www.promptingguide.ai/">Prompting Guide</a> by DAIR.AI (2022)</dt>
<dd>
Great guide on prompting.
</dd>
<dt><a href="https://arxiv.org/abs/2002.12327">A Primer in BERTology: What we know about how BERT works</a> by Anna Rogers, Olga Kovaleva, and Anna Rumshisky (2020)</dt>
<dd>
A fairly comprehensive review of the BERT language models that create embeddings of text.
</dd>
<dt><a href="https://www.bertforhumanists.org/tutorials/">BERT for Humanists</a> by Matt Wilkens, David Mimno, Melanie Walsh, Rosamond Thalken, and Maria Antoniak (2022)</dt>
<dd>
Fantastic tutorial on language models, geared toward those that do not have a background in the area
</dd>
</dl>
<p><a href="https://github.com/Mooler0410/LLMsPracticalGuide">The Practical Guides for Large Language Models</a></p>
<h2 id="diffusion-models">Diffusion Models</h2>
<dl>
<dt><a href="https://www.youtube.com/watch?v=cS6JQpEY9cs">Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications</a> by Karsten Kreis, Ruiqi Gao, and Arash Vahdat (2022)</dt>
<dd>
Video tutorial on diffusion models.
</dd>
</dl>
<!-- [Diffusion models are autoencoders](https://sander.ai/2022/01/31/diffusion.html) by Sander Dieleman (2022) -->
<!-- [Diffusion Models: A Practical Guide](https://scale.com/guides/diffusion-models-guide) by Vivek Muppalla and Sean Hendryx (2022) -->
<!-- 
[What are Diffusion Models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) by Lilian Weng (2021)
: A much more technical explanation of diffusion models. -->
<dl>
<dt><a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a> by Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli (2015)</dt>
<dd>
The paper that introduced diffusion models.
</dd>
</dl>
<p><a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a> by Yang Song, Stefano Ermon (2019)</p>
<p><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a> by Jonathan Ho, Ajay Jain, Pieter Abbeel (2020)</p>
<p><a href="https://arxiv.org/abs/2107.00630">Variational Diffusion Models</a> by Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho (2021)</p>
<!-- [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456) by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole (2020) -->
<!-- [Diffusion language models](https://sander.ai/2023/01/09/diffusion-language.html) by Sander Dieleman (2023)
: Blog post on using diffusion models for language modeling.
 -->
<h2 id="training-data-extraction">Training data extraction</h2>
<dl>
<dt><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">Extracting Training Data from Large Language Models</a> by Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel (2021), <a href="https://bair.berkeley.edu/blog/2020/12/20/lmmem/" class="inline">blog</a>, <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting" class="inline">video</a></dt>
<dd>
Original paper showcasing extracting training data from large language models.
</dd>
<dt><a href="https://aclanthology.org/2022.acl-long.577/">Deduplicating Training Data Makes Language Models Better</a> by Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini (2022)</dt>
<dd>
Duplicates in the data continue to be the most easily identifiable reason for memorization.
</dd>
<dt><a href="https://arxiv.org/abs/2202.07646">Quantifying Memorization Across Neural Language Models</a> by Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang (2022)</dt>
<dd>
A more careful and comprehensive study showing larger models memorize more.
</dd>
<dt><a href="https://arxiv.org/abs/2301.13188">Extracting Training Data from Diffusion Models</a> by Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace (2023)</dt>
<dd>
It’s also possible to extract training data from diffusion models.
</dd>
</dl>
<p><a href="https://arxiv.org/abs/2212.03860">Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models</a> by Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein (2022)</p>
<h2 id="membership-inference">Membership Inference</h2>
<dl>
<dt><a href="https://ieeexplore.ieee.org/abstract/document/7958568">Membership Inference Attacks against Machine Learning Models</a> by Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov (2017)</dt>
<dd>
First paper introducing the idea of membership inference.
</dd>
<dt><a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles</a> by Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer (2021)</dt>
<dd>
Current best membership inference attack.
</dd>
<dt><a href="http://proceedings.mlr.press/v139/choquette-choo21a/choquette-choo21a.pdf">Label-Only Membership Inference Attacks</a> by Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot (2021)</dt>
<dd>
Masking outputs does not prevent membership inference.
</dd>
<dt><a href="https://arxiv.org/abs/1802.04889">Understanding Membership Inferences on Well-Generalized Learning Models</a> by Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen</dt>
<dd>
Outliers can be more vulnerable to membership inference.
</dd>
</dl>
<h2 id="differential-privacy">Differential Privacy</h2>
<p><a href="https://proceedings.neurips.cc/paper/2020/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a> by Matthew Jagielski, Jonathan Ullman, and Alina Oprea (2020)</p>
<!-- [Deep Learning with Differential Privacy](https://dl.acm.org/doi/abs/10.1145/2976749.2978318) by Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang (2016)

[Toward Training at ImageNet Scale with Differential Privacy](https://arxiv.org/abs/2201.12328) by Alexey Kurakin, Shuang Song, Steve Chien, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta (2022)
 -->
<dl>
<dt><a href="https://arxiv.org/abs/2212.06470">Considerations for Differentially Private Learning with Large-Scale Public Pretraining</a> by Florian Tramèr, Gautam Kamath, Nicholas Carlini (2022)</dt>
<dd>
Privacy is hard. Publicly accessible data is not the same as nor equivalent to public data. Differential privacy has limitations. Public data <em>looks</em> different from private data in meaningful ways, but our benchmarks sometimes miss that.
</dd>
</dl>
<p><a href="https://aclanthology.org/2022.findings-acl.171.pdf">Training Text-to-Text Transformers with Privacy Guarantees</a> by Natalia Ponomareva Jasmijn Bastings Sergei Vassilvitskii (2022)</p>
<dl>
<dt><a href="https://arxiv.org/abs/2303.00654">How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy</a> by Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Thakurta (2023)</dt>
<dd>
Practical guide to implementing DP.
</dd>
<dt><a href="https://arxiv.org/abs/2211.06530">Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning</a> by Christopher A. Choquette-Choo, H. Brendan McMahan, Keith Rush, and Abhradeep Thakurta</dt>
<dd>
State-of-the-art privacy mechanism.
</dd>
</dl>
<h2 id="ethics">Ethics</h2>
<!-- [Algorithms that remember: model inversion attacks and data protection law](https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0083) by Michael Veale, Reuben Binns, and Lilian Edwards (2018)
 -->
<p><a href="https://blog.allenai.org/using-large-language-models-with-care-eeb17b0aed27">Using Large Language Models With Care</a> by Maria Antoniak, Li Lucy, Maarten Sap, and Luca Soldaini (2023)</p>
<p><a href="https://epic.org/wp-content/uploads/2023/05/EPIC-Generative-AI-White-Paper-May2023.pdf">Generating Harms: Generative AI’s Impact and Path Forwards</a> by Epic.org (2023)</p>
<dl>
<dt><a href="https://dl.acm.org/doi/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</a> by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell (2021)</dt>
<dd>
One of the earliest published works on possible harms in large language models
</dd>
</dl>
<p><a href="https://arxiv.org/abs/2102.02503">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</a> by Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli (2021)</p>
<p><a href="https://arxiv.org/abs/2112.04359">Ethical and social risks of harm from Language Models</a> by Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel (2021)</p>
<dl>
<dt><a href="https://arxiv.org/abs/2202.05338">Accountability in an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learning</a> by A. Feder Cooper, Emanuel Moss, Benjamin Laufer and Helen Nissenbaum (2022)</dt>
<dd>
A synthesized analysis of why accountability is so hard/elusive for AI/ML systems
</dd>
</dl>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3311957.3359435">Contestability in Algorithmic Systems</a> by Kristen Vaccaro, Karrie Karahalios, Deirdre K. Mulligan, Daniel Kluttz, and Tad Hirsch (2019)</p>
<h2 id="ongoing-litigation">Ongoing litigation</h2>
<p><a href="https://clarksonlawfirm.com/wp-content/uploads/2023/06/0001.-2023.06.28-OpenAI-Complaint.pdf">Clarkson, et al. v. OpenAI, et al.</a> OpenAI class action lawsuit (scraping)</p>
<p><a href="https://torrentfreak.com/images/authors-vs-openai.pdf">Paul Tremblay and Mona Awad v. OpenAI, et al.</a> OpenAI class action lawsuit (copyright infringement)</p>
<p><a href="https://githubcopilotlitigation.com/">Doe 1, et al. v. GitHub, Inc., et al.</a> GitHub Copilot class action lawsuit</p>
<p><a href="https://stablediffusionlitigation.com/">Andersen, et al. v. Stability AI Ltd., et al.</a> Stable Diffusion class action lawsuit</p>
<h2 id="in-the-news">In the news</h2>
<p><a href="https://docs.google.com/spreadsheets/d/11Ps8ILDHH-vojJGyIx7CcaoB5l1mBRHy3OQAgWkm0W4/edit#gid=0">AI Ethics &amp; Policy News</a> compiled by Casey Fiesler</p>
<p><a href="https://www.fastcompany.com/90906560/adobe-feels-so-confident-its-firefly-generative-ai-wont-breach-copyright-itll-cover-your-legal-bills">Adobe is so confident its Firefly generative AI won’t breach copyright that it’ll cover your legal bills</a> by Chris Stokel-Walker for <em>Fast Company</em> (2023)</p>
<p><a href="https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers">Microsoft Is Bringing OpenAI’s GPT-4 AI model to US Government Agencies</a> by Rachel Metz for <em>Bloomberg News</em> (2023)</p>
<p><a href="https://www.sagaftra.org/sag-aftra-national-board-unanimously-agrees-send-authorization-vote-members">SAG-AFTRA National Board Unanimously Agrees To Send Authorization Vote To Members</a> <em>SAG-AFTRA</em> (2023)</p>
<p><a href="https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt">Samsung workers made a major error by using ChatGPT</a> by Lewis Maddison for <em>TechRadar</em> (2023)</p>
<p><a href="https://arstechnica.com/tech-policy/2023/04/openai-may-be-sued-after-chatgpt-falsely-says-aussie-mayor-is-an-ex-con/">OpenAI threatened with landmark defamation lawsuit over ChatGPT false claims</a> by Ashley Belanger for <em>Ars Technica</em> (2023)</p>
<p><a href="https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit">Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content</a> by James Vincent for <em>The Verge</em> (2023)</p>
<p><a href="https://ipkitten.blogspot.com/2023/02/uk-government-axes-plans-to-broaden.html">UK Government axes plans to broaden existing text and data mining exception</a> by Eleonora Rosati for <em>IPKat</em> (2023)</p>
<p><a href="https://copyrightblog.kluweriplaw.com/2022/08/24/the-uk-government-moves-forward-with-a-text-and-data-mining-exception-for-all-purposes/">The UK government moves forward with a text and data mining exception for all purposes</a> by Alina Trapova and João Pedro Quintais for <em>Kluwer Copyright Blog</em> (2022)</p>
<p><a href="https://www.project-disco.org/intellectual-property/011823-israel-ministry-of-justice-issues-opinion-supporting-the-use-of-copyrighted-works-for-machine-learning/">Israel Ministry of Justice Issues Opinion Supporting the Use of Copyrighted Works for Machine Learning</a> by Jonathan Band for <em>The Disruptive Competition Project</em> (2023)</p>
<p><a href="https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E">Copyright in generative deep learning</a> by Giorgio Franceschelli and Mirco Musolesi for <em>Data &amp; Policy</em> (2022)</p>
<p><a href="https://publishingperspectives.com/2023/03/ai-at-bologna-the-hair-raising-topic-of-2023/">‘AI’ at Bologna: The Hair-Raising Topic of 2023?</a> by Porter Anderson for <em>Publishing Perspectives</em> (2023)</p>
<p><a href="https://www2.law.ucla.edu/volokh/ailibel.pdf">Large Libel Models? Liability for AI Output</a> by Eugene Volokh (draft 2023)</p>
<p><a href="https://www.lawfareblog.com/section-230-wont-protect-chatgpt">Section 230 Won’t Protect ChatGPT</a> by Matt Perault (2023)</p>
<p><a href="https://www.technologyreview.com/2023/04/19/1071789/openais-hunger-for-data-is-coming-back-to-bite-it/">OpenAI’s hunger for data is coming back to bite it</a> by Melissa Heikkilä (2023)</p>
</main>

<footer>
<p class="signoff">
  <a href="https://genlaw.github.io/">← GenLaw</a>
</p>
</footer>

<script>
document.addEventListener("DOMContentLoaded", function () {
    // Non-essential if user has JavaScript off. Just hides TOC button on scroll.
    const nav = document.querySelector("nav");
    let lastScrollTop = 0;
    const min_diff_px = 32;
    
    function didScroll() {
        const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
        if (currentScrollTop < lastScrollTop) {
            nav.classList.add("scrolled-up");
            nav.classList.remove("scrolled-down");
            lastScrollTop = currentScrollTop;
        } else if (currentScrollTop > lastScrollTop + min_diff_px) {
            nav.classList.remove("scrolled-up");
            nav.classList.add("scrolled-down");
            lastScrollTop = currentScrollTop;
        }
    }

    window.addEventListener("scroll", didScroll);
});
  
document.addEventListener("DOMContentLoaded", function () {
    const headings = document.querySelectorAll('main h1, main h2, main h3, main h4');

    function handleIntersection(entries) {
        //  IntersectionObserver's entries are ordered by their position in the DOM tree
        const topmostEntry = entries.find(entry => entry.isIntersecting);
        console.log(topmostEntry)
        if (!topmostEntry) return;

        const tocElementId = 'toc-' + topmostEntry.target.id;
        const tocElement = document.getElementById(tocElementId);
        if (!tocElement) return;

        const otherTocElements = document.querySelectorAll('.active');
        otherTocElements.forEach(el => el.classList.remove('active'));
        tocElement.classList.add('active');
    }

    // root: null -> entire browser viewport
    const options = {
        root: null,
        rootMargin: '0px',
        threshold: 0.8
    };
    const observer = new IntersectionObserver(handleIntersection, options);

    headings.forEach(heading => {
        observer.observe(heading);
    });

    // Manually trigger the IntersectionObserver callback for the initial state
    const initialEntries = Array.from(headings).map(heading => ({
        isIntersecting: heading.getBoundingClientRect().top < window.innerHeight && heading.getBoundingClientRect().bottom > 0,
        target: heading
    }));
    handleIntersection(initialEntries);
});
document.addEventListener('DOMContentLoaded', function() {
    const nav_anchors = document.querySelectorAll('nav a');
    const contents_checkbox = document.getElementById('contents');
  
    nav_anchors.forEach(anchor => {
      anchor.addEventListener('click', function(event) {
        // Do not stop normal functionality of the anchor tag
        // event.preventDefault();
  
        // Uncheck the input with id "contents"
        if (contents_checkbox && contents_checkbox.type === 'checkbox') {
          contents_checkbox.checked = false;
        }
      });
    });
  });
  
document.addEventListener('DOMContentLoaded', () => {
  const headings = document.querySelectorAll('main h1[id], main h2[id], main h3[id], main h4[id], main h5[id], main h6[id]');

  headings.forEach(heading => {
    heading.addEventListener('click', event => {
      const target = event.target;

      if (target.tagName.toLowerCase().startsWith('h') && target.hasAttribute('id')) {
        const headingId = target.getAttribute('id');
        const url = new URL(window.location.href);
        url.hash = headingId;

        navigator.clipboard.writeText(url.toString())
          .then(() => {
            console.log('Heading URL copied to clipboard:', url.toString());
            target.classList.add('copy-success');
            target.setAttribute('title', 'Copied URL to clipboard! ✅');
            setTimeout(() => {
              target.classList.remove('copy-success');
              target.removeAttribute('title');
            }, 3000);

          })
          .catch(err => {
            console.error('Failed to copy the heading URL:', err);
            target.classList.add('copy-error');
            target.setAttribute('title', 'Failed to copy URL! ❌');
            setTimeout(() => {
              target.classList.remove('copy-error');
              target.removeAttribute('title');
            }, 3000);

          });
      }
    });
  });
});
</script>

</body>
</html>
