<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="icon" type="image/svg+xml" href="/favicon.svg">
    <link rel="icon" type="image/png" href="/favicon.png">
    

    <title>Report of the 1st Workshop on Generative AI and Law [blog]</title>
        <link rel="stylesheet" href="styles.css">
    
    <!-- Google Analytics tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W2ZW2ZM1M6"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-W2ZW2ZM1M6');
    </script>
</head>
<body>
    <header>
        <img src="/favicon.ico" alt="reading robot icon">
        <a href="index.html" style="white-space: nowrap;">GenLaw '23</a>
        <nav>
            <a href="index.html#schedule">Schedule</a>
            <a href="cfp.html">CFP</a>
            <a href="papers.html">Accepted Papers</a>
            <a href="resources.html">Resources</a>
            <a href="glossary.html">Glossary</a>
        </nav>
    </header>
    <main id="">
                <aside class="sidebar-container">
            <div class="sidebar" id="table-of-contents">
                <ul>
                <li><a href="#section2" id="toc-section2">The Impact of Generative AI on Law</a></li>
                <li><a href="#section3" id="toc-section3">Developing a Shared Knowledge Base</a>
                <ul>
                <li><a href="#crafting-glossaries-and-metaphors" id="toc-crafting-glossaries-and-metaphors">Crafting Glossaries and Metaphors</a></li>
                <li><a href="#understanding-evolving-business-models" id="toc-understanding-evolving-business-models"><em>Understanding Evolving Business Models</em></a></li>
                </ul></li>
                <li><a href="#section4" id="toc-section4">Pinpointing Unique Aspects of GenAI</a></li>
                <li><a href="#section5" id="toc-section5">A Preliminary Taxonomy of Legal Issues</a></li>
                <li><a href="#section6" id="toc-section6">Toward a Long-Term Research Agenda</a></li>
                <li><a href="#section7" id="toc-section7">What’s next for GenLaw?</a></li>
                <li><a href="#references" id="toc-references">References</a></li>
                </ul>
            </div>
        </aside>
                <div id="content">
            <h1>Report of the 1st Workshop on Generative AI and Law [blog]</h1>
            <p>The <a href="2023-full-report.html">Report of the 1st Workshop on Generative AI and Law</a> reflects the synthesis of ideas from our Day 2 roundtables. The report begins with a brief framing note about the impact of <a href="#section2">Generative AI on Law</a>, then goes on to suggest useful components of a <a href="#section3">shared knowledge base</a>, an outline of ways that <a href="#section4">Generative AI is unique</a> (and the ways it isn’t), a preliminary <a href="#section5">taxonomy</a> of legal issues, and a concrete <a href="#section6">research agenda</a> at the intersection of Generative AI and law.<br />
            The report closes with some <a href="#section7">brief takeaways</a> about this emerging field.</p>
            <h1 id="section2">The Impact of Generative AI on Law</h1>
            <p><em><a href="2023-full-report.html#sec:vision">Section 2</a></em></p>
            <p>We begin the report with some background that helps situate why Generative AI is going to have such an impact on law. It’s true that Generative AI is “generative” because it generates text, images, audio, or other types of output. But it is also “generative” in the sense of Jonathan Zittrain’s theory of generative technologies from 2008: it has the “capacity to produce unanticipated change through unfiltered contributions from broad and varied audiences”<span class="citation" data-cites="zittrainfuture">(Zittrain 2008)</span>. It provides enormous leverage across a wide range of tasks, is readily built on by a huge range of users, and facilitates rapid iterative improvement as those users share their innovations with each other. As a result, generative-AI systems will be both immensely societally significant — too significant for governments to ignore or to delay dealing with — and present an immensely broad range of legal issues.</p>
            <h1 id="section3">Developing a Shared Knowledge Base</h1>
            <p><em><a href="2023-full-report.html#sec:knowledge">Section 3</a></em></p>
            <h2 id="crafting-glossaries-and-metaphors">Crafting Glossaries and Metaphors</h2>
            <p>At GenLaw, it quickly became apparent that the two communities may share words, but these words may not share meanings. We therefore developed a <a href="glossary.html">glossary</a> and list of <a href="metaphors.html">metaphors</a> to make definitions of terms concrete, so that we can be sure that we’re talking about the same things (and talking about them precisely) Drawing from the <a href="2023-full-report.html">report</a>, we use “harm” as an example of an overloaded term — on that has a colloquial understanding that can be mistaken for a term-of-art in the law:</p>
            <blockquote>
            <p>Many technologists were not aware of the importance of <a href="glossary.html#harm">harms</a> as a specific and consequential concept in law, rather than a general, non-specific notion of unfavorable outcomes. We found our way to common understandings only over the course of our conversations, and often only after many false starts.</p>
            </blockquote>
            <p>We also suggest that <a href="metaphors.html">metaphors</a> can be a useful abstraction for communication between communities, since they play a central role to how both machine-learning experts and lawyers communicate among themselves. Lawyers use metaphors as rational frameworks for thinking through the relevant similarities and differences between cases. In the machine-learning community, experts use metaphors all the time to give intuitions for technical processes. For example, generative models are said to “<a href="metaphors.html#learning">learn</a>” or “<a href="metaphors.html#collage">make collages</a>” of training data. This imaginative naming is often intentional; technical processes are often named for the human behaviors or science-fiction tropes that inspired them.</p>
            <p>However, we caution that metaphors can also simplify and distort (as is the case with the metaphor of a <a href="metaphors.html#collage">collage</a>). For better communication across fields, it can nevertheless be instructive to understand the ways a metaphor is appropriate and where it falls short.</p>
            <h2 id="understanding-evolving-business-models"><em>Understanding Evolving Business Models</em></h2>
            <p><em><a href="2023-full-report.html#sec:business">Section 3.3</a></em></p>
            <p>Generative AI is not a single entity or business model. There are many different types of Generative AI built by a diversity of actors, potentially in partnership. To get a better understanding of the array of generative-AI systems and the ways that they’re produced, we can look at existing and emerging business models: 1) business-to-consumer (B2C) hosted services (including direct to consumer applications, e.g., OpenAI’s ChatGPT, Google’s Bard) and <a href="glossary.html#api">application programming interfaces (APIs)</a>); 2) business-to-business (B2B) integration with hosted services ( via direct partnership/integration or through the use of APIs), 3) products derived from <a href="glossary.html#app:os">open-source</a> software, models, and datasets (e.g., ,some versions of Stable Diffusion, offered by Stability AI, are open sourced), and 4) companies that operate at specific points in the generative-AI <a href="glossary.html#supply-chain">supply chain</a> <span class="citation" data-cites="lee2023talkin">(Lee, Cooper, and Grimmelmann 2023)</span> (e.g., companies that work specifically on <a href="glossary.html#datasets">datasets</a>, training diagnostics, and <a href="glossary.html#training">training</a> and deployment). We go into more detail on each of these in the report.</p>
            <h1 id="section4">Pinpointing Unique Aspects of GenAI</h1>
            <p><em><a href="2023-full-report.html#sec:challenges">Section 4</a></em></p>
            <p>With so many different types of generative-AI systems, some of the lawyers in the room asked the ML experts to clarify some commonalities that make the “magic” of Generative AI. We discussed three aspects:</p>
            <ol type="1">
            <li><strong>Open-ended tasks:</strong> Generative AI models are trained with open-ended tasks in mind, rather than narrowly-defined tasks. This means that the same model could be used for translating between languages as could be used for question answering.</li>
            <li><strong>Multi-stage pipelines:</strong> In part as a result of training with open-ended tasks, models are trained in a <em>multi-stage training pipeline</em> containing stages like: <a href="glossary.html#pre-training-and-fine-tuning">pre-training, fine-tuning</a>, and <a href="glossary.html#alignment">alignment</a> (e.g., <a href="glossary.html#reinforcement-learning">RLHF</a>). The delineations between these different stages is flexible, but the result is to create <a href="glossary.html#base-model">base models</a> that have a “base” of knowledge about the world within the model. This training pipeline is part of a larger supply chain, which further contributes to novel dynamics in the production and use of generative-AI systems <span class="citation" data-cites="lee2023talkin">(Lee, Cooper, and Grimmelmann 2023)</span>.</li>
            <li><strong>Scale:</strong> Finally, arguably the most discussed element of Generative AI was the role of <em>scale</em>: <em>scale</em> of datasets, of models, of the number of generations, and of compute.</li>
            </ol>
            <blockquote>
            <dl>
            <dt><a href="glossary.html#pre-training-and-fine-tuning">Pre-training</a></dt>
            <dd>
            One of the <em>ah-ha</em> moments we had at the workshop was when we realized that technologists and legal scholars understood the term <em>pre-training</em> to mean very different things. Technologists use the term pre-training to refer to an early, general-purpose phase of the model training process, but legal scholars assumed that the term referred to a data preparation stage prior to and independent of training. Clearing up that confusion made the importance of pre-training models much more apparent.
            </dd>
            </dl>
            </blockquote>
            <h1 id="section5">A Preliminary Taxonomy of Legal Issues</h1>
            <p><em><a href="2023-full-report.html#sec:taxonomy">Section 5</a></em></p>
            <p>We also outlined some of the legal issues Generative AI raises. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Not all issues that Generative AI raises are <em>new</em>. Generative AI can be used to perform many tasks for which other AI/ML technology has already been commonly used.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In the report, we focus on four areas where Generative AI raises novel challenges for the law: intent, privacy, misinformation and disinformation, and intellectual property.</p>
            <ul>
            <li><strong>Intent:</strong> Generative AI can cause harms that are similar to those brought about by human actors, but do so without human intention.</li>
            <li><strong>Privacy:</strong> Generative AI presents new privacy challenges and complicates existing ones. Models are trained on large-scale datasets that may contain all sorts of private information (e.g., <a href="glossary.html#pii">PII</a>), which in turn can be memorized and then leaked in generations <span class="citation" data-cites="brown2022privacy carlini2023extracting">(Brown et al. 2022; Carlini et al. 2023)</span>.</li>
            <li><strong>Misinformation and Disinformation:</strong> Contending with misinformation and disinformation online is not a new challenge. However, easy, cheap generation disinformation about individuals (e.g., deepfakes) could contribute to new types of harms.</li>
            <li><strong>Intellectual Property:</strong> The copyright issues raised by generative-AI systems are all over the news.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> But, Generative AI raises other issues in <a href="glossary.html#ip">intellectual property</a> more broadly. For example, it raises numerous questions around patents,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> volition (see below), market externalities, trade secrecy, scraping, and more.</li>
            </ul>
            <p>For more detail on each of these, please see the <a href="2023-report.html#sec:taxonomy">report</a>.</p>
            <blockquote>
            <dl>
            <dt>Volition</dt>
            <dd>
            Human volition plays an important and subtle role in defining IP infringement. For example, copyright infringement normally requires that a human intentionally made a copy of a protected work, but not that the human was consciously aware that they were infringing. Generative-AI systems may occasionally produce outputs that look like duplicates of the training data. Some participants at GenLaw were concerned that it may be easy to deflect the role of human-made design choices by making such choices seem “internal” to the system (when, in fact, such choices are typically not foregone conclusions or strict technical requirements).
            </dd>
            </dl>
            </blockquote>
            <h1 id="section6">Toward a Long-Term Research Agenda</h1>
            <p><em><a href="2023-full-report.html#sec:agenda">Section 6</a></em></p>
            <p>Through our discussions, we elicited several important and promising research directions. Each of these directions brings forth challenges that require engagement from law and machine-learning experts, and likely many other disciplines as well.</p>
            <ol type="1">
            <li><p><strong>Centralization versus Decentralization:</strong> First, who will build the components of future generative-AI systems? How centralized or decentralized will these actors be? This is as much a technical question as it is a business and legal question. Technical constraints, such as the design of a dataset, inform the logistics of the supply chain: who builds the component, what gets built, and how. Improvements in synthetic data may enable well-resourced actors to generate their own training data. Existing and emerging business models create incentives for particular modes of interaction among players. Finally, every important potential bottleneck in Generative AI – from datasets to compute to models and beyond – will be the focus of close scrutiny. These questions cannot be discussed intelligently without contributions from both technical and legal scholars.</p></li>
            <li><p><strong>Rules, Standards, Reasonableness, and Best Practices</strong>: In some cases, we have standards of care. For example, HIPAA strictly regulates which kinds of data are treated as personally identifying and subject to stringent security standards. In other cases, we rely on <em>reasonable</em> standards of practice; but, what is <em>reasonable</em> is often both context-dependent and evolving.. What is <em>reasonable</em> will depend on technical advancements and constraints. This is an area where we feel that collaboration between legal and technical experts and policymakers is urgently needed.</p></li>
            <li><p><strong>Notice and Takedown ≠ Machine Unlearning:</strong> Notice and takedown requests are particularly challenging for generative-AI models. The impact of each example in the training data is dispersed throughout the model once it is trained and cannot be easily traced. There are entire subfields of machine learning devoted to problems like these, such as machine unlearning and attribution. However, both machine unlearning and attribution are very young fields, and their strategies are (for the most part) not yet computationally feasible to implement in practice for deployed generative-AI systems. There is, of course, intense (and growing) investment in this area.</p></li>
            <li><p><strong>Evaluation Metrics:</strong> Effective ways to evaluate generative-AI systems currently remain elusive. System capabilities and harms are not readily quantifiable; designing useful metrics will be an important, related area of research for Generative AI and law (and will also, in turn, influence what we understand to be <em>reasonable</em> system behaviors).</p></li>
            </ol>
            <h1 id="section7">What’s next for GenLaw?</h1>
            <p>As is clear from the diversity of issues discussed within these topics, it is difficult to pithily sum up the main takeaways of GenLaw. (Nevertheless, we attempt to do so in the <a href="2023-report.html">report</a>.)</p>
            <p>To close here, we just want to say that we’re so thrilled to have been able to have hosted the first GenLaw, host this discussion and for the community that’s grown up around it. We hope that you refer to and share our report, <a href="glossary.html">glossary</a>, <a href="metaphors.html">metaphors</a>, <a href="resources.html">resources</a>, <a href="talkin.html">law review article</a>, and <a href="explainers.html">explainers series</a> (written for and with workshop participants) as reference material for your own learning, teaching materials, and research.</p>
            <p>Right now, we’re growing GenLaw into a nonprofit, which will be a home for research, education, and interdisciplinary discussion. We will continue to create resources for both a general audience and subject-matter experts. So far, we’ve brought together experts across Generative AI, law, policy, and other computer-science disciplines from 25 different institutions, and we are excited to continue engaging with experts across industry, academia, and government. While our first event and materials have had a U.S.- based orientation, we are actively focusing on expanding our engagement globally.</p>
            <p>Stay tuned for more from us. You can subscribe to updates <a href="https://groups.google.com/g/genlaw-community">here</a>.</p>
            <p>And, of course, a big thank you to our sponsors: Google, Microsoft, Schmidt Futures, OpenAI, Anthropic, Cornell Law School, and ML Collective.</p>
            <h1 class="unnumbered" id="references">References</h1>
            <div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
            <div id="ref-brown2022privacy" class="csl-entry" role="listitem">
            Brown, Hannah, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. 2022. <span>“<span class="nocase">What Does It Mean for a Language Model to Preserve Privacy?</span>”</span> In <em>Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2280–92. FAccT ’22. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3531146.3534642">https://doi.org/10.1145/3531146.3534642</a>.
            </div>
            <div id="ref-CallisonBurch_2023" class="csl-entry" role="listitem">
            Callison-Burch, Chris. 2023. <span>“<span class="nocase">Understanding Generative Artificial Intelligence and Its Relationship to Copyright</span>.”</span> University of Pennsylvania, School of Engineering; Applied Sciences, Department of Computer; Information Science; Testimony before The U.S. House of Representatives Judiciary Committee, Subcommittee on Courts, Intellectual Property, and the Internet.
            </div>
            <div id="ref-carlini2023extracting" class="csl-entry" role="listitem">
            Carlini, Nicholas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. <span>“<span class="nocase">Extracting Training Data from Diffusion Models</span>.”</span> <a href="https://arxiv.org/abs/2301.13188">https://arxiv.org/abs/2301.13188</a>.
            </div>
            <div id="ref-epic" class="csl-entry" role="listitem">
            Fergusson, Grant, Caitriona Fitzgerald, Chris Frascella, Megan Iorio, Tom McBrien, Calli Schroeder, Ben Winters, and Enid Zhou. 2023. <span>“<span class="nocase">Generating Harms: Generative AI’s Impact &amp; Paths Forward</span>.”</span> Electronic Privacy Information Center.
            </div>
            <div id="ref-lee2023talkin" class="csl-entry" role="listitem">
            Lee, Katherine, A. Feder Cooper, and James Grimmelmann. 2023. <span>“<span class="nocase">Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain</span>.”</span> <em>arXiv Preprint arXiv:2309.08133</em>.
            </div>
            <div id="ref-lipton2023privacy" class="csl-entry" role="listitem">
            Lipton, Zachary. 2023. <span>“<span class="nocase">My Statement to the US Senate AI Insight Forum on Privacy and Liability</span>.”</span> <a href="https://www.abridge.com/blog/ai-policy-conversation">https://www.abridge.com/blog/ai-policy-conversation</a>.
            </div>
            <div id="ref-sag2023safety" class="csl-entry" role="listitem">
            Sag, Matthew. 2023. <span>“<span class="nocase">Copyright Safety for Generative AI</span>.”</span> <em>Houston Law Review</em>.
            </div>
            <div id="ref-samuelson" class="csl-entry" role="listitem">
            Samuelson, Pamela. 2023. <span>“<span class="nocase">Generative AI meets copyright</span>.”</span> <em>Science</em> 381 (6654): 158–61. <a href="https://doi.org/10.1126/science.adi0656">https://doi.org/10.1126/science.adi0656</a>.
            </div>
            <div id="ref-vyas2023provable" class="csl-entry" role="listitem">
            Vyas, Nikhil, Sham Kakade, and Boaz Barak. 2023. <span>“<span class="nocase">On Provable Copyright Protection for Generative Models</span>.”</span> <a href="https://arxiv.org/abs/2302.10870">https://arxiv.org/abs/2302.10870</a>.
            </div>
            <div id="ref-zittrainfuture" class="csl-entry" role="listitem">
            Zittrain, Jonathan. 2008. <em>The Future of the Internet–and How to Stop It</em>. USA: Yale University Press.
            </div>
            </div>
            <section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
            <hr />
            <ol>
            <li id="fn1"><p>We focus on the workshop’s intended scope of <a href="glossary.html#app:privacy">privacy</a> and <a href="glossary.html#ip">intellectual property (IP)</a> issues. This is by no means a comprehensive taxonomy of legal issues nor of harms (legally cognizable or otherwise). Other reports have made significant attempts to catalog such harms from Generative AI, for example <span class="citation" data-cites="epic">Fergusson et al. (2023)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn2"><p>For example, instead of using a purpose-built sentiment-analysis model, one might simply prompt an LLM with labeled examples of text and ask it to classify text of interest; one could use a trained LLM to answer questions with “yes” or “no” answers (i.e., to perform classification).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn3"><p>Much has been written and said on copyright and Generative AI, e.g., <span class="citation" data-cites="lee2023talkin">Lee, Cooper, and Grimmelmann (2023)</span>, <span class="citation" data-cites="sag2023safety">Sag (2023)</span>, <span class="citation" data-cites="samuelson">Samuelson (2023)</span>, <span class="citation" data-cites="CallisonBurch_2023">Callison-Burch (2023)</span>, <span class="citation" data-cites="vyas2023provable">Vyas, Kakade, and Barak (2023)</span>, <span class="citation" data-cites="lipton2023privacy">Lipton (2023)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn4"><p>Regarding inventorship, how should ownership rights for a drug designed using generative-AI tools be allocated?<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            </ol>
            </section>
        </div>
    </main>
    <footer>
        <p style="white-space: nowrap;">GenLaw '23</p>
        <p class="colophon">Built with <a href="http://pandoc.org">Pandoc</a> and <a href="https://chat.openai.com/chat?model=gpt-4">ChatGPT</a> :)</p>
    </footer>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
           const toc = document.getElementById('table-of-contents');
           if (toc == null) {
            console.log('no toc');
            return;
           }
           // hide all level 2 LIs until clicked
           const level2LIs = toc.querySelectorAll('#table-of-contents > ul > li > ul > li');
           level2LIs.forEach(li => {
               const child_ul = li.querySelector('ul');
               child_ul.style.display = 'none';

               const triangle = document.createElement('span');
               triangle.classList.add('triangle');
               triangle.innerHTML = '&#9658;';
               triangle.onclick = () => {
                   if (child_ul.style.display == 'none') {
                       child_ul.style.display = 'block';
                       triangle.innerHTML = '&#9660;';
                   } else {
                       child_ul.style.display = 'none';
                       triangle.innerHTML = '&#9658;';
                   }
               };
               li.prepend(triangle);
           });
        });
    </script>
</body>
</html>
